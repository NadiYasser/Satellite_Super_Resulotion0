{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd81851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n",
      "\n",
      " DATA LOADED:\n",
      "  Train: 64800 samples\n",
      "  Val:   8100 samples\n",
      "  Test:  8100 samples\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os \n",
    "os.chdir(\"/home/jadli/Bureau/BDAI2/Satellite_Super_Resulotion0\")\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from importlib import reload\n",
    "import src.utils.config\n",
    "reload(src.utils.config)\n",
    "from src.utils.config import CONFIG\n",
    "\n",
    "from src.utils.data_loader import create_loaders\n",
    "# from src.utils.models_architecture import         \n",
    "from src.utils.helper_functions import train_sr, val_sr, plot_sr_progress\n",
    "from src.utils.train_model_sr import train_model_sr\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device : {device}\")\n",
    "\n",
    "# CONFIG FROM YAML \n",
    "data_root      = CONFIG[\"paths\"][\"output_root\"]\n",
    "batch_size     = 1 #CONFIG[\"training\"][\"batch_size\"]\n",
    "num_workers    = CONFIG[\"training\"][\"num_workers\"]\n",
    "use_aug        = CONFIG[\"training\"].get(\"use_augmentation\", True)\n",
    "\n",
    "# HYPERPARAMS FROM CONFIG \n",
    "lr              = 1e-4 #CONFIG[\"training\"][\"lr\"]\n",
    "weight_decay    = CONFIG[\"training\"][\"weight_decay\"]\n",
    "num_epochs      = 1 # CONFIG[\"training\"][\"epochs\"]\n",
    "step_size       = CONFIG[\"training\"][\"scheduler_step_size\"]\n",
    "gamma           = CONFIG[\"training\"][\"scheduler_gamma\"]\n",
    "\n",
    "\n",
    "# === DataLoaders ===\n",
    "\n",
    "train_loader, val_loader, test_loader = create_loaders(\n",
    "    root=data_root,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    use_augmentation=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd67b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockHR(nn.Module):\n",
    "    def __init__(self, channels=128, scale=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.conv1(x)\n",
    "        res = self.relu(res)\n",
    "        res = self.conv2(res)\n",
    "        return x + res * self.scale\n",
    "\n",
    "\n",
    "class EDSRPro(nn.Module):\n",
    "    \"\"\"\n",
    "    Version lourde : upsample d'abord → residual blocks en HR.\n",
    "    Entrée  : LR (B,3,H,W)\n",
    "    Sortie  : HR (B,3,scale*H, scale*W)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_blocks=32, channels=128, scale_factor=4):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "        # 1) Feature extraction en LR\n",
    "        self.conv_head = nn.Conv2d(3, channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # 2) Upsampling learned → HR features\n",
    "        up_layers = []\n",
    "        if scale_factor in [2, 4]:\n",
    "            for _ in range(scale_factor // 2):\n",
    "                up_layers += [\n",
    "                    nn.Conv2d(channels, channels * 4, 3, padding=1),\n",
    "                    nn.PixelShuffle(2),\n",
    "                    nn.ReLU(True),\n",
    "                ]\n",
    "        self.upsample = nn.Sequential(*up_layers)\n",
    "\n",
    "        # 3) Residual blocks en HR\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlockHR(channels=channels) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "        # 4) Global skip en HR\n",
    "        self.conv_tail = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # 5) Reconstruction finale\n",
    "        self.conv_last = nn.Conv2d(channels, 3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: LR\n",
    "        x = self.conv_head(x)         # LR features\n",
    "        x = self.upsample(x)          # HR features\n",
    "        x_head_hr = x                 # pour skip global\n",
    "\n",
    "        x_res = self.res_blocks(x)    # HR residual blocks\n",
    "        x = self.conv_tail(x_res) + x_head_hr\n",
    "        x = self.conv_last(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ca4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training EDSR_PRO from scratch\n",
      "No previous training history found.\n",
      "\n",
      " [EDSR_PRO] Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 361/64800 [00:50<2:28:42,  7.22it/s, loss=0.0342, psnr=25.4] "
     ]
    }
   ],
   "source": [
    "model = EDSRPro().to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "best_model_path = CONFIG[\"model\"][\"best_EDSR_PRO_path\"]\n",
    "history_path = CONFIG[\"history\"][\"EDSR_PRO_history_path\"]\n",
    "\n",
    "train_model_sr(\n",
    "    model=model,\n",
    "    model_name=\"EDSR_PRO\",\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=num_epochs,\n",
    "    scale_factor=4,\n",
    "    model_requires_upscale=False,\n",
    "    best_model_path=best_model_path,\n",
    "    history_path=history_path,\n",
    "    use_amp=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20846f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BDAI2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
