{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd81851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n",
      "\n",
      "ðŸ“¦ DATA LOADED:\n",
      "  Train: 64800 samples\n",
      "  Val:   8100 samples\n",
      "  Test:  8100 samples\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os \n",
    "os.chdir(\"/home/jadli/Bureau/BDAI2/Satellite_Super_Resulotion0\")\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from importlib import reload\n",
    "import src.utils.config\n",
    "reload(src.utils.config)\n",
    "from src.utils.config import CONFIG\n",
    "\n",
    "from src.utils.data_loader import create_loaders\n",
    "from src.models.models_architecture import SRCNN        \n",
    "from src.utils.helper_functions import train_sr, test_sr, plot_sr_progress\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device : {device}\")\n",
    "\n",
    "# CONFIG FROM YAML \n",
    "data_root      = CONFIG[\"paths\"][\"output_root\"]\n",
    "batch_size     = CONFIG[\"training\"][\"batch_size\"]\n",
    "num_workers    = CONFIG[\"training\"][\"num_workers\"]\n",
    "use_aug        = CONFIG[\"training\"].get(\"use_augmentation\", True)\n",
    "\n",
    "# HYPERPARAMS FROM CONFIG \n",
    "lr              = CONFIG[\"training\"][\"lr\"]\n",
    "weight_decay    = CONFIG[\"training\"][\"weight_decay\"]\n",
    "num_epochs      = CONFIG[\"training\"][\"epochs\"]\n",
    "step_size       = CONFIG[\"training\"][\"scheduler_step_size\"]\n",
    "gamma           = CONFIG[\"training\"][\"scheduler_gamma\"]\n",
    "\n",
    "\n",
    "# === DataLoaders ===\n",
    "\n",
    "train_loader, val_loader, test_loader = create_loaders(\n",
    "    root=data_root,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    use_augmentation=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842dae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockHR(nn.Module):\n",
    "    def __init__(self, channels=128, scale=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.conv1(x)\n",
    "        res = self.relu(res)\n",
    "        res = self.conv2(res)\n",
    "        return x + res * self.scale\n",
    "\n",
    "\n",
    "class EDSRPro(nn.Module):\n",
    "    \"\"\"\n",
    "    Version lourde : upsample d'abord â†’ residual blocks en HR.\n",
    "    EntrÃ©e  : LR (B,3,H,W)\n",
    "    Sortie  : HR (B,3,scale*H, scale*W)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_blocks=32, channels=128, scale_factor=4):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "        # 1) Feature extraction en LR\n",
    "        self.conv_head = nn.Conv2d(3, channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # 2) Upsampling learned â†’ HR features\n",
    "        up_layers = []\n",
    "        if scale_factor in [2, 4]:\n",
    "            for _ in range(scale_factor // 2):\n",
    "                up_layers += [\n",
    "                    nn.Conv2d(channels, channels * 4, 3, padding=1),\n",
    "                    nn.PixelShuffle(2),\n",
    "                    nn.ReLU(True),\n",
    "                ]\n",
    "        self.upsample = nn.Sequential(*up_layers)\n",
    "\n",
    "        # 3) Residual blocks en HR\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlockHR(channels=channels) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "        # 4) Global skip en HR\n",
    "        self.conv_tail = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # 5) Reconstruction finale\n",
    "        self.conv_last = nn.Conv2d(channels, 3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: LR\n",
    "        x = self.conv_head(x)         # LR features\n",
    "        x = self.upsample(x)          # HR features\n",
    "        x_head_hr = x                 # pour skip global\n",
    "\n",
    "        x_res = self.res_blocks(x)    # HR residual blocks\n",
    "        x = self.conv_tail(x_res) + x_head_hr\n",
    "        x = self.conv_last(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f315098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Hyperparams EDSR Pro ===\n",
    "scale_factor = 4\n",
    "num_blocks   = 32         # lourd : 32, tu peux tester 16 si OOM\n",
    "channels     = 128        # 128 recommandÃ© pour Pro\n",
    "batch_size   = 4           # <= 8 sinon OOM probable\n",
    "num_epochs   = 10\n",
    "lr           = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "# === Model ===\n",
    "model = EDSRPro(num_blocks=num_blocks, channels=channels, scale_factor=scale_factor).to(device)\n",
    "\n",
    "criterion  = nn.L1Loss()\n",
    "optimizer  = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler  = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_psnrs,  val_psnrs  = [], []\n",
    "best_psnr = 0.0\n",
    "best_model_path = CONFIG[\"model\"][\"best_EDSR_path\"]\n",
    "os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
    "\n",
    "scaler = None  # pour AMP\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    train_loss, train_psnr, scaler = train_sr(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        loss_fn=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scale_factor=scale_factor,\n",
    "        model_requires_upscale=False,   # IMPORTANT pour EDSRPro\n",
    "        scheduler=scheduler,\n",
    "        use_amp=True,\n",
    "        scaler=scaler\n",
    "    )\n",
    "\n",
    "    val_loss, val_psnr = test_sr(\n",
    "        model=model,\n",
    "        test_loader=val_loader,\n",
    "        loss_fn=criterion,\n",
    "        device=device,\n",
    "        scale_factor=scale_factor,\n",
    "        model_requires_upscale=False\n",
    "    )\n",
    "\n",
    "    # if val_psnr > best_psnr:\n",
    "    #     best_psnr = val_psnr\n",
    "    #     torch.save(model.state_dict(), best_model_path)\n",
    "    #     print(f\" New best model saved with Val PSNR = {best_psnr:.2f} dB\")\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_psnrs.append(train_psnr)\n",
    "    val_psnrs.append(val_psnr)\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.6f} | Train PSNR: {train_psnr:.2f} dB\")\n",
    "    print(f\"Val   loss: {val_loss:.6f} | Val   PSNR: {val_psnr:.2f} dB\")\n",
    "    print(f\"  âž¤ LR: {optimizer.param_groups[0]['lr']:.8f}\")\n",
    "\n",
    "plot_sr_progress(train_losses, val_losses, train_psnrs, val_psnrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses, val_losses = [], []\n",
    "# train_psnrs,  val_psnrs  = [], []\n",
    "\n",
    "# best_psnr = 0.0\n",
    "# best_model_path = CONFIG[\"model\"][\"best_EDSR_path\"]\n",
    "# os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    \n",
    "#     train_loss, train_psnr, scaler = train_sr(\n",
    "#         model=model,\n",
    "#         train_loader=train_loader,\n",
    "#         loss_fn=criterion,\n",
    "#         optimizer=optimizer,\n",
    "#         device=device,\n",
    "#         scale_factor=4,                 \n",
    "#         model_requires_upscale=False,   \n",
    "#         scheduler=scheduler\n",
    "#     )\n",
    "\n",
    "#     val_loss, val_psnr = test_sr(\n",
    "#         model=model,\n",
    "#         test_loader=val_loader,\n",
    "#         loss_fn=criterion,\n",
    "#         device=device,\n",
    "#         scale_factor=4,\n",
    "#         model_requires_upscale=False\n",
    "#     )\n",
    "\n",
    "#     if val_psnr > best_psnr:\n",
    "#         best_psnr = val_psnr\n",
    "#         torch.save(model.state_dict(), best_model_path)\n",
    "#         print(f\" New best model saved with Val PSNR = {best_psnr:.2f} dB\")\n",
    "\n",
    "#     train_losses.append(train_loss)\n",
    "#     val_losses.append(val_loss)\n",
    "#     train_psnrs.append(train_psnr)\n",
    "#     val_psnrs.append(val_psnr)\n",
    "\n",
    "#     print(f\"Train loss: {train_loss:.6f} | Train PSNR: {train_psnr:.2f} dB\")\n",
    "#     print(f\"Val   loss: {val_loss:.6f} | Val   PSNR: {val_psnr:.2f} dB\")\n",
    "#     print(f\"  -> LR: {optimizer.param_groups[0]['lr']:.8f}\")\n",
    "\n",
    "# plot_sr_progress(train_losses, val_losses, train_psnrs, val_psnrs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BDAI2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
