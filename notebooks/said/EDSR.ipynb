{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f079f3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "device : cuda\n",
      "\n",
      " DATA LOADED:\n",
      "  Train: 64800 samples\n",
      "  Val:   8100 samples\n",
      "  Test:  8100 samples\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os \n",
    "os.chdir(\"/home/jadli/Bureau/BDAI2/Satellite_Super_Resulotion0\")\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from importlib import reload\n",
    "import src.utils.config\n",
    "reload(src.utils.config)\n",
    "from src.utils.config import CONFIG\n",
    "\n",
    "from src.utils.data_loader import create_loaders\n",
    "from src.models.models_architecture import SRCNN        \n",
    "from src.utils.helper_functions import train_sr, val_sr, plot_sr_progress\n",
    "\n",
    "best_model_path = CONFIG[\"model\"][\"best_EDSR_path\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device : {device}\")\n",
    "\n",
    "# CONFIG FROM YAML \n",
    "data_root      = CONFIG[\"paths\"][\"output_root\"]\n",
    "batch_size     = CONFIG[\"training\"][\"batch_size\"]\n",
    "num_workers    = CONFIG[\"training\"][\"num_workers\"]\n",
    "use_aug        = CONFIG[\"training\"].get(\"use_augmentation\", True)\n",
    "\n",
    "# HYPERPARAMS FROM CONFIG \n",
    "lr              = CONFIG[\"training\"][\"lr\"]\n",
    "weight_decay    = CONFIG[\"training\"][\"weight_decay\"]\n",
    "num_epochs      = CONFIG[\"training\"][\"epochs\"]\n",
    "step_size       = CONFIG[\"training\"][\"scheduler_step_size\"]\n",
    "gamma           = CONFIG[\"training\"][\"scheduler_gamma\"]\n",
    "\n",
    "\n",
    "# LOAD DATA \n",
    "\n",
    "train_loader, val_loader, test_loader = create_loaders(\n",
    "    root=data_root,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    use_augmentation=use_aug\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f65d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels=64, scale=0.1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x) * self.scale\n",
    "\n",
    "\n",
    "class EDSR(nn.Module):\n",
    "    def __init__(self, num_blocks=32, scale_factor=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Initial Feature Extractor\n",
    "        self.conv_head = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # 2) Residual Blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(64) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "        # 3) Global skip\n",
    "        self.conv_tail = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # 4) Upsampling\n",
    "        up_layers = []\n",
    "        if scale_factor == 2 or scale_factor == 4:\n",
    "            for _ in range(scale_factor // 2):\n",
    "                up_layers += [\n",
    "                    nn.Conv2d(64, 256, kernel_size=3, padding=1),\n",
    "                    nn.PixelShuffle(2),\n",
    "                    nn.ReLU(True)\n",
    "                ]\n",
    "        self.upsample = nn.Sequential(*up_layers)\n",
    "\n",
    "        # 5) Reconstruction\n",
    "        self.conv_last = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_head = self.conv_head(x)\n",
    "        x_res = self.res_blocks(x_head)\n",
    "        x = self.conv_tail(x_res) + x_head   \n",
    "        x = self.upsample(x)\n",
    "        x = self.conv_last(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9360a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training from scratch\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# TRAIN \u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m train_loss, train_psnr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_requires_upscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# VALIDATION \u001b[39;00m\n\u001b[1;32m     51\u001b[0m val_loss, val_psnr \u001b[38;5;241m=\u001b[39m val_sr(\n\u001b[1;32m     52\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     53\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     model_requires_upscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m~/Bureau/BDAI2/Satellite_Super_Resulotion0/src/utils/helper_functions.py:66\u001b[0m, in \u001b[0;36mtrain_sr\u001b[0;34m(model, train_loader, loss_fn, optimizer, device, scale_factor, model_requires_upscale, scheduler, use_amp, scaler)\u001b[0m\n\u001b[1;32m     63\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     64\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 66\u001b[0m batch_psnr \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_psnr\u001b[49m\u001b[43m(\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     69\u001b[0m epoch_psnr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_psnr\n",
      "File \u001b[0;32m~/Bureau/BDAI2/Satellite_Super_Resulotion0/src/utils/helper_functions.py:17\u001b[0m, in \u001b[0;36mcalc_psnr\u001b[0;34m(sr, hr)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mCompute PSNR (Peak Signal-to-Noise Ratio)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m mse \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(sr, hr)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mse \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog10(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m mse\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = EDSR().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_psnrs,  val_psnrs  = [], []\n",
    "\n",
    "best_psnr = 0.0\n",
    "best_model_path = CONFIG[\"model\"][\"best_EDSR_path\"]\n",
    "os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
    "\n",
    "# LOAD CHECKPOINT (IF EXISTS)\n",
    "start_epoch = 0\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(\"Loading checkpoint:\", best_model_path)\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "\n",
    "    if \"model\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "        best_psnr = checkpoint[\"best_psnr\"]\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\" Resuming training from epoch {start_epoch} | Best PSNR = {best_psnr:.2f}\")\n",
    "    else:\n",
    "        print(\" Old checkpoint without optimizer/scheduler. Loading model only.\")\n",
    "        model.load_state_dict(checkpoint)\n",
    "else:\n",
    "    print(\" Training from scratch\")\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # TRAIN \n",
    "    train_loss, train_psnr = train_sr(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        loss_fn=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scale_factor=4,\n",
    "        model_requires_upscale=False,   \n",
    "        scheduler=scheduler\n",
    "    )\n",
    "\n",
    "    # VALIDATION \n",
    "    val_loss, val_psnr = val_sr(\n",
    "        model=model,\n",
    "        val_loader=val_loader,\n",
    "        loss_fn=criterion,\n",
    "        device=device,\n",
    "        scale_factor=4,\n",
    "        model_requires_upscale=False\n",
    "    )\n",
    "\n",
    "    # SAVE BEST MODEL \n",
    "    if val_psnr > best_psnr:\n",
    "        best_psnr = val_psnr\n",
    "\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_psnr\": best_psnr,\n",
    "        }, best_model_path)\n",
    "\n",
    "        print(f\" New BEST model saved at epoch {epoch+1} with PSNR = {best_psnr:.2f}\")\n",
    "\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_psnrs.append(train_psnr)\n",
    "    val_psnrs.append(val_psnr)\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.6f} | Train PSNR: {train_psnr:.2f} dB\")\n",
    "    print(f\"Val   loss: {val_loss:.6f} | Val   PSNR: {val_psnr:.2f} dB\")\n",
    "    print(f\"â†’ Current LR: {optimizer.param_groups[0]['lr']:.8f}\")\n",
    "\n",
    "# PLOT\n",
    "plot_sr_progress(train_losses, val_losses, train_psnrs, val_psnrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BDAI2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
